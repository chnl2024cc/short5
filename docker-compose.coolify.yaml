# Coolify Docker Compose Configuration
# Note: Coolify automatically creates and manages networks for this compose file.
# Services can communicate using Docker service names as hostnames (e.g., 'postgres', 'redis').
# No explicit network definitions needed - Coolify handles network configuration automatically.

services:
  # PostgreSQL Database (Internal Service)
  postgres:
    image: postgres:15-alpine
    container_name: short5_postgres
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-short5_user}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB:-short5_db}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./database/schema.sql:/docker-entrypoint-initdb.d/01-schema.sql:ro
    restart: unless-stopped
    # Internal service - not exposed via Traefik
    healthcheck:
      test:
        - CMD-SHELL
        - 'pg_isready -U $${POSTGRES_USER} -d $${POSTGRES_DB}'
      interval: 5s
      timeout: 20s
      retries: 10

  # Redis for Celery/RQ (Internal Service)
  redis:
    image: redis:7-alpine
    container_name: short5_redis
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    # Internal service - not exposed via Traefik

  # FastAPI Backend (Exposed via Traefik)
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: short5_backend
    environment:
      DATABASE_URL: postgresql://${POSTGRES_USER:-short5_user}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB:-short5_db}
      REDIS_URL: redis://redis:6379/0
      # Use SERVICE_URL_BACKEND from Coolify for the backend URL
      BACKEND_BASE_URL: ${SERVICE_URL_BACKEND}
      ENVIRONMENT: ${ENVIRONMENT:-production}
      JWT_SECRET_KEY: ${JWT_SECRET_KEY}
      JWT_ALGORITHM: ${JWT_ALGORITHM:-HS256}
      JWT_ACCESS_TOKEN_EXPIRE_MINUTES: ${JWT_ACCESS_TOKEN_EXPIRE_MINUTES:-30}
      JWT_REFRESH_TOKEN_EXPIRE_DAYS: ${JWT_REFRESH_TOKEN_EXPIRE_DAYS:-7}
      CELERY_BROKER_URL: redis://redis:6379/0
      CELERY_RESULT_BACKEND: redis://redis:6379/0
      # CORS origins - should include frontend URL
      CORS_ORIGINS: ${SERVICE_URL_FRONTEND},${SERVICE_URL_FRONTEND}
    volumes:
      - backend_uploads:/app/uploads
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      video_worker:
        condition: service_healthy
    # Production command (no reload)
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000
    healthcheck:
      test: ["CMD-SHELL", "python -c 'import socket; socket.create_connection((\"localhost\", 8000))' || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 30s

  # FFmpeg Video Worker (Internal Service)
  video_worker:
    build:
      context: ./video_worker
      dockerfile: Dockerfile
    container_name: short5_video_worker
    environment:
      DATABASE_URL: postgresql://${POSTGRES_USER:-short5_user}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB:-short5_db}
      REDIS_URL: redis://redis:6379/0
      CELERY_BROKER_URL: redis://redis:6379/0
      CELERY_RESULT_BACKEND: redis://redis:6379/0
      C_FORCE_ROOT: "true"
    volumes:
      - video_worker_temp:/tmp/video_processing
      - backend_uploads:/app/uploads
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    command: celery -A worker worker --loglevel=info --concurrency=${WORKER_CONCURRENCY:-2} -Q celery
    healthcheck:
      test: ["CMD-SHELL", "celery -A worker inspect ping | grep -q pong || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped

  # Celery Flower - Monitoring (Optional: Internal or with Auth)
  flower:
    build:
      context: ./flower
      dockerfile: Dockerfile
    container_name: short5_flower
    environment:
      CELERY_BROKER_URL: redis://redis:6379/0
      CELERY_RESULT_BACKEND: redis://redis:6379/0
      FLOWER_BASIC_AUTH: ${FLOWER_USER:-admin}:${FLOWER_PASSWORD}
    depends_on:
      - redis
      - video_worker
    command: celery flower --port=5555 --broker=redis://redis:6379/0 --address=0.0.0.0
    restart: unless-stopped

  # Nuxt Frontend (Exposed via Traefik)
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
      args:
        # Build arguments - use .env variables or Coolify's SERVICE_URL_BACKEND
        # These are baked into the build and cannot be changed at runtime
        NUXT_PUBLIC_API_BASE_URL: ${SERVICE_URL_BACKEND}/api/v1
        NUXT_PUBLIC_BACKEND_BASE_URL: ${SERVICE_URL_BACKEND}
        NODE_ENV: ${NODE_ENV:-production}
    container_name: short5_frontend
    environment:
      # Runtime environment variables (configurable via .env or PaaS)
      NODE_ENV: ${NODE_ENV:-production}
      HOST: 0.0.0.0
      PORT: 3000
      NUXT_TELEMETRY_DISABLED: 1
      # Note: NUXT_PUBLIC_* variables are build-time only in Nuxt 3/4
      # They must be set via build args above
    depends_on:
      backend:
        condition: service_healthy
    healthcheck:
      # Use Node.js for health check (available in the image)
      # Checks if the server responds with HTTP 200
      test: ["CMD-SHELL", "node -e \"require('http').get('http://localhost:3000', (r) => {process.exit(r.statusCode === 200 ? 0 : 1)}).on('error', () => process.exit(1))\" || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped

  pgadmin:
    image: 'dpage/pgadmin4:latest'
    environment:
      - SERVICE_URL_PGADMIN
      - 'PGADMIN_DEFAULT_EMAIL=${PGADMIN_DEFAULT_EMAIL:?}'
      - 'PGADMIN_DEFAULT_PASSWORD=${PGADMIN_DEFAULT_PASSWORD:?}'
    volumes:
      - 'pgadmin-data:/var/lib/pgadmin'
    healthcheck:
      test:
        - CMD
        - wget
        - '-qO-'
        - 'http://localhost:80/login'
      interval: 5s
      timeout: 10s
      retries: 5
    depends_on:
      - postgres

volumes:
  postgres_data:
  redis_data:
  backend_uploads:
  video_worker_temp:

